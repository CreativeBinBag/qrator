{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5Uja39gZmD1",
        "outputId": "e4fd7d7a-c3d0-4b9b-89d5-e02de1d27437"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.12/dist-packages (0.11.7)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: pdfminer.six==20250506 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (20250506)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (11.3.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (4.30.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
            "Setup Complete. Libraries are ready.\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries. pdfplumber is excellent for robust PDF text and table extraction.\n",
        "!pip install requests pdfplumber\n",
        "\n",
        "import requests\n",
        "import xml.etree.ElementTree as ET\n",
        "import pdfplumber\n",
        "import re\n",
        "from pathlib import Path\n",
        "import os\n",
        "from collections import defaultdict\n",
        "import html\n",
        "\n",
        "print(\"Setup Complete. Libraries are ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# arXiv API Ingestor\n",
        "\n",
        "def fetch_recent_quant_ph_papers(max_results=20):\n",
        "    \"\"\"\n",
        "    Fetches the most recent papers from the arXiv quant-ph category.\n",
        "    Returns a list of dictionaries, each representing a paper.\n",
        "    \"\"\"\n",
        "    base_url = 'http://export.arxiv.org/api/query?'\n",
        "    query = f'search_query=cat:quant-ph&sortBy=submittedDate&sortOrder=descending&max_results={max_results}'\n",
        "\n",
        "    print(f\"Fetching {max_results} recent papers from quant-ph...\")\n",
        "    try:\n",
        "        response = requests.get(base_url + query)\n",
        "        response.raise_for_status()  # Raises an exception for bad status codes\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching from arXiv: {e}\")\n",
        "        return []\n",
        "\n",
        "    root = ET.fromstring(response.content)\n",
        "    papers = []\n",
        "    # arXiv API uses a namespace, which we need to handle\n",
        "    namespace = {'arxiv': 'http://www.w3.org/2005/Atom'}\n",
        "\n",
        "    for entry in root.findall('arxiv:entry', namespace):\n",
        "        paper_id = entry.find('arxiv:id', namespace).text.split('/abs/')[-1]\n",
        "        title = entry.find('arxiv:title', namespace).text.strip().replace('\\n', ' ')\n",
        "        pdf_url = entry.find('arxiv:link[@title=\"pdf\"]', namespace).attrib['href']\n",
        "\n",
        "        papers.append({\n",
        "            'id': paper_id,\n",
        "            'title': title,\n",
        "            'pdf_url': pdf_url\n",
        "        })\n",
        "\n",
        "    print(f\"Successfully fetched {len(papers)} papers.\")\n",
        "    return papers\n",
        "\n",
        "# --- Execute and Display ---\n",
        "papers_to_analyze = fetch_recent_quant_ph_papers()\n",
        "# Display the first few fetched papers to show it works\n",
        "for i, paper in enumerate(papers_to_analyze[:10]):\n",
        "    print(f\"{i+1}. {paper['title']} ({paper['id']})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jefg3iA2bAkx",
        "outputId": "c5d57b50-04a7-4a7c-9284-ab8c1f190eeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching 20 recent papers from quant-ph...\n",
            "Successfully fetched 20 papers.\n",
            "1. Strong-to-Weak Symmetry Breaking Phases in Steady States of Quantum   Operations (2509.09669v1)\n",
            "2. Bogoliubov quasi-particles in superconductors are integer-charged   particles inapplicable for braiding quantum information (2509.09663v1)\n",
            "3. Towards A High-Performance Quantum Data Center Network Architecture (2509.09653v1)\n",
            "4. Resource quantification for programming low-depth quantum circuits (2509.09642v1)\n",
            "5. Work statistics of sudden Quantum quenches: A random matrix theory   perspective on Gaussianity and its deviations (2509.09640v1)\n",
            "6. Reconstructing the Hamiltonian from the local density of states using   neural networks (2509.09604v1)\n",
            "7. Fault-tolerant transformations of spacetime codes (2509.09603v1)\n",
            "8. PT symmetry-enriched non-unitary criticality (2509.09587v1)\n",
            "9. Quantum signatures of proper time in optical ion clocks (2509.09573v1)\n",
            "10. Vacuum electromagnetic field correlations between two moving points (2509.09557v1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PDF Processing Engine\n",
        "\n",
        "# Create a directory to store downloaded PDFs\n",
        "PDF_DIR = Path('downloaded_pdfs')\n",
        "PDF_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "def download_and_extract_text(paper):\n",
        "    \"\"\"\n",
        "    Downloads a PDF and extracts its full text.\n",
        "    Returns the text content as a single string.\n",
        "    \"\"\"\n",
        "    pdf_path = PDF_DIR / f\"{paper['id']}.pdf\"\n",
        "\n",
        "    # Download the file if it doesn't exist\n",
        "    if not pdf_path.exists():\n",
        "        try:\n",
        "            print(f\"Downloading {paper['id']}...\")\n",
        "            response = requests.get(paper['pdf_url'])\n",
        "            response.raise_for_status()\n",
        "            pdf_path.write_bytes(response.content)\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"  -> Failed to download {paper['id']}: {e}\")\n",
        "            return None, \"\"\n",
        "\n",
        "    # Extract text using pdfplumber\n",
        "    full_text = \"\"\n",
        "    try:\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            print(f\"  -> Extracting text from {paper['id']}...\")\n",
        "            for page in pdf.pages:\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    full_text += page_text + \"\\n\"\n",
        "        print(f\"  -> Extracted {len(full_text)} characters.\")\n",
        "        return pdf_path, full_text\n",
        "    except Exception as e:\n",
        "        print(f\"  -> Failed to extract text from {paper['id']}: {e}\")\n",
        "        return pdf_path, \"\"\n",
        "\n",
        "# --- We will call this function inside the main loop later ---\n",
        "# This block just defines the capability."
      ],
      "metadata": {
        "id": "uBqYpFoic3Zv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The Heuristic Scoring Engine\n",
        "\n",
        "# This helper function is used by Tiers 1 & 2\n",
        "def get_evidence_snippets(text, keyword, window=150):\n",
        "    \"\"\"Finds a keyword and returns a snippet of text around it.\"\"\"\n",
        "    snippets = []\n",
        "    # Use finditer to get match objects, which have start/end positions\n",
        "    for match in re.finditer(r'\\b' + re.escape(keyword) + r'\\b', text, re.IGNORECASE):\n",
        "        start = max(0, match.start() - window)\n",
        "        end = min(len(text), match.end() + window)\n",
        "        snippet = text[start:end]\n",
        "        # Highlight the keyword in the snippet\n",
        "        highlighted = snippet.replace(match.group(0), f\"<strong>{match.group(0)}</strong>\")\n",
        "        snippets.append(f\"...{html.escape(highlighted)}...\")\n",
        "    return snippets\n",
        "\n",
        "def score_paper(text):\n",
        "    \"\"\"\n",
        "    Analyzes the text of a paper and returns a score and evidence.\n",
        "    This version includes the refined \"SMART Tier 3\" for caption analysis.\n",
        "    \"\"\"\n",
        "    score = 0\n",
        "    evidence = defaultdict(list)\n",
        "\n",
        "    # --- Tier 1: Weighted Keyword Search ---\n",
        "    strong_keywords = {\n",
        "        \"state-of-the-art\": 25, \"fidelity\": 15, \"benchmark\": 20,\n",
        "        \"quantum volume\": 25, \"error rate\": 15, \"gate fidelity\": 20,\n",
        "        \"cross-entropy\": 15\n",
        "    }\n",
        "    platform_keywords = {\n",
        "        \"IBMQ\": 10, \"Sycamore\": 10, \"Quantinuum\": 10, \"IonQ\": 10,\n",
        "        \"Rigetti\": 10\n",
        "    }\n",
        "    all_keywords = {**strong_keywords, **platform_keywords}\n",
        "\n",
        "    for keyword, weight in all_keywords.items():\n",
        "        count = len(re.findall(r'\\b' + re.escape(keyword) + r'\\b', text, re.IGNORECASE))\n",
        "        if count > 0:\n",
        "            score += weight * count\n",
        "            # Get the first snippet as evidence\n",
        "            evidence[f\"Tier 1: Found '{keyword}' ({count}x)\"].extend(get_evidence_snippets(text, keyword, 75)[:1])\n",
        "\n",
        "    # --- Tier 2: Contextual Analysis  ---\n",
        "    # This regex is forgiving about whitespace and section formatting.\n",
        "    # It looks for a heading, followed by ANY amount of whitespace (\\s+),\n",
        "    # and stops when it SEES (but doesn't consume) the next likely heading.\n",
        "    section_finder_regex = r'(?:results|experiment|discussion|conclusion|benchmark)\\s+(.*?)(?=\\n\\s*\\n\\s*(?:[IVX\\d]+\\.|references|acknowledgments|appendix)|\\Z)'\n",
        "    results_sections = re.findall(section_finder_regex, text, re.DOTALL | re.IGNORECASE)\n",
        "    context_text = \" \".join(results_sections)\n",
        "\n",
        "    if context_text:\n",
        "        for keyword, weight in strong_keywords.items():\n",
        "            count = len(re.findall(r'\\b' + re.escape(keyword) + r'\\b', context_text, re.IGNORECASE))\n",
        "            if count > 0:\n",
        "                score += (weight * 1.5) * count # 50% bonus for being in a key section\n",
        "                evidence[f\"Tier 2: Found '{keyword}' in Results/Experiment\"].extend(get_evidence_snippets(context_text, keyword, 75)[:1])\n",
        "\n",
        "    # --- Tier 3: SMART Structural Analysis ---\n",
        "    # This logic analyzes the CAPTIONS of tables and figures, not just their mentions.\n",
        "\n",
        "    # Regex to find a declaration (e.g., \"Table 1\") and capture its following caption text (up to 5 lines).\n",
        "    caption_regex = r'(Table\\s+[IVX\\d]+|Figure\\s+\\d+|Fig\\.\\s+\\d+)\\.?\\s*((?:[^\\n]+\\n?){1,5})'\n",
        "\n",
        "    # Keywords specifically sought after within captions. These indicate data and comparison.\n",
        "    caption_keywords = {\n",
        "        \"benchmark\": 50, \"comparison\": 40, \"performance\": 40,\n",
        "        \"fidelity\": 30, \"error rate\": 30, \"summary of results\": 50\n",
        "    }\n",
        "\n",
        "    captions_found = re.findall(caption_regex, text, re.IGNORECASE)\n",
        "\n",
        "    if captions_found:\n",
        "        # First, add a small base score for the raw count, as high density is still a useful signal.\n",
        "        num_tables = sum(1 for cap in captions_found if 'table' in cap[0].lower())\n",
        "        num_figures = len(captions_found) - num_tables\n",
        "        score += num_tables * 5\n",
        "        score += num_figures * 2\n",
        "        evidence[\"Tier 3: Base Count\"].append(f\"Found {num_tables} Tables and {num_figures} Figures.\")\n",
        "\n",
        "        # Second, the \"smart\" part: analyze the caption text itself for high-value keywords.\n",
        "        for entity, caption_text in captions_found: # e.g., entity=\"Table 1\", caption_text=\"Summary of...\"\n",
        "            for keyword, weight in caption_keywords.items():\n",
        "                if re.search(r'\\b' + re.escape(keyword) + r'\\b', caption_text, re.IGNORECASE):\n",
        "                    # Give a significant score bonus if the keyword is in a TABLE caption\n",
        "                    final_weight = weight * 1.5 if 'table' in entity.lower() else weight\n",
        "                    score += final_weight\n",
        "\n",
        "                    # Create a rich evidence snippet from the actual caption\n",
        "                    clean_caption = ' '.join(caption_text.strip().splitlines())\n",
        "                    # Highlight the keyword found within the caption for the report\n",
        "                    highlighted_caption = re.sub(r'(' + re.escape(keyword) + r')', r'<strong>\\1</strong>', clean_caption, flags=re.IGNORECASE)\n",
        "\n",
        "                    evidence_key = f\"Tier 3 SMART: Found '{keyword}' in caption for {entity.strip()}\"\n",
        "                    evidence[evidence_key].append(f\"{html.escape(highlighted_caption)}\")\n",
        "\n",
        "                    # Break after finding the first keyword to avoid score inflation from a single caption.\n",
        "                    break\n",
        "\n",
        "    return int(score), evidence"
      ],
      "metadata": {
        "id": "j53ljMKZc8KF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Digest Generator and Main Execution Loop\n",
        "\n",
        "def generate_html_digest(scored_papers, filename=\"benchmark_digest.html\"):\n",
        "    \"\"\"Generates a static HTML file from the scored papers.\"\"\"\n",
        "    # Sort papers by score, descending\n",
        "    sorted_papers = sorted(scored_papers, key=lambda p: p['score'], reverse=True)\n",
        "\n",
        "    html_content = \"\"\"\n",
        "    <html>\n",
        "    <head>\n",
        "        <title>Metriq Benchmark Digest</title>\n",
        "        <style>\n",
        "            body { font-family: sans-serif; margin: 2em; }\n",
        "            .paper { border: 1px solid #ccc; padding: 1em; margin-bottom: 1em; border-radius: 5px; }\n",
        "            h2 { font-size: 1.2em; }\n",
        "            .score { font-size: 1.5em; font-weight: bold; color: #0056b3; }\n",
        "            .score-bar { background-color: #e9ecef; border-radius: 3px; }\n",
        "            .score-bar-inner { background-color: #007bff; height: 10px; border-radius: 3px; }\n",
        "            .evidence-list { list-style-type: none; padding-left: 0; }\n",
        "            .evidence-list li { background-color: #f0f0f0; padding: 0.5em; margin-top: 0.5em; border-radius: 3px; font-family: monospace; font-size: 0.9em;}\n",
        "            strong { color: #d9534f; }\n",
        "        </style>\n",
        "    </head>\n",
        "    <body>\n",
        "        <h1>Metriq Benchmark Digest</h1>\n",
        "        <p>Generated for recent quant-ph submissions. Top papers ranked by their likelihood of containing benchmark results.</p>\n",
        "    \"\"\"\n",
        "\n",
        "    for paper in sorted_papers:\n",
        "      # In the loop for each paper\n",
        "        max_score = sorted_papers[0]['score'] if sorted_papers else 1 # Avoid division by zero\n",
        "        score_percentage = (paper['score'] / max_score) * 100 if max_score > 0 else 0\n",
        "\n",
        "        html_content += f\"\"\"\n",
        "        <div class=\"paper\">\n",
        "            <h2><a href=\"http://arxiv.org/abs/{paper['id']}\" target=\"_blank\">{html.escape(paper['title'])}</a></h2>\n",
        "            <p>arXiv ID: {paper['id']}</p>\n",
        "            <p class=\"score\">Likelihood Score: {paper['score']}</p>\n",
        "            <div class=\"score-bar\"><div class=\"score-bar-inner\" style=\"width: {score_percentage}%;\"></div></div>\n",
        "            <h3>Evidence:</h3>\n",
        "            <ul class=\"evidence-list\">\n",
        "        \"\"\"\n",
        "        for reason, snippets in paper['evidence'].items():\n",
        "            for snippet in snippets:\n",
        "                html_content += f\"<li><b>{html.escape(reason)}:</b> {snippet}</li>\"\n",
        "\n",
        "        html_content += \"</ul></div>\"\n",
        "\n",
        "    html_content += \"</body></html>\"\n",
        "\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        f.write(html_content)\n",
        "\n",
        "    print(f\"\\nDigest generated! See the file '{filename}' in the file browser on the left.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Fetch papers\n",
        "    papers = fetch_recent_quant_ph_papers(max_results=20) # keeping it small for a demo\n",
        "\n",
        "    scored_results = []\n",
        "\n",
        "    # Process and score each paper\n",
        "    for paper in papers:\n",
        "        print(\"-\" * 20)\n",
        "        pdf_path, text = download_and_extract_text(paper)\n",
        "        if text:\n",
        "            score, evidence = score_paper(text)\n",
        "            print(f\"  -> Final Score for {paper['id']}: {score}\")\n",
        "            paper_data = paper.copy()\n",
        "            paper_data['score'] = score\n",
        "            paper_data['evidence'] = evidence\n",
        "            scored_results.append(paper_data)\n",
        "        else:\n",
        "            print(f\"  -> Skipping {paper['id']} due to processing error.\")\n",
        "\n",
        "    # Generate the final report\n",
        "    if scored_results:\n",
        "        generate_html_digest(scored_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdYcwFGudG3K",
        "outputId": "6ff99f80-3f38-4dd6-84c6-4af16cad8eb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching 20 recent papers from quant-ph...\n",
            "Successfully fetched 20 papers.\n",
            "--------------------\n",
            "  -> Extracting text from 2509.09669v1...\n",
            "  -> Extracted 141869 characters.\n",
            "  -> Final Score for 2509.09669v1: -2347\n",
            "--------------------\n",
            "  -> Extracting text from 2509.09663v1...\n",
            "  -> Extracted 36575 characters.\n",
            "  -> Final Score for 2509.09663v1: -119\n",
            "--------------------\n",
            "  -> Extracting text from 2509.09653v1...\n",
            "  -> Extracted 28818 characters.\n",
            "  -> Final Score for 2509.09653v1: 670\n",
            "--------------------\n",
            "  -> Extracting text from 2509.09642v1...\n",
            "  -> Extracted 65592 characters.\n",
            "  -> Final Score for 2509.09642v1: -660\n",
            "--------------------\n",
            "  -> Extracting text from 2509.09640v1...\n",
            "  -> Extracted 51013 characters.\n",
            "  -> Final Score for 2509.09640v1: -318\n",
            "--------------------\n",
            "  -> Extracting text from 2509.09604v1...\n",
            "  -> Extracted 42299 characters.\n",
            "  -> Final Score for 2509.09604v1: 3\n",
            "--------------------\n",
            "  -> Extracting text from 2509.09603v1...\n",
            "  -> Extracted 195290 characters.\n",
            "  -> Final Score for 2509.09603v1: -2632\n",
            "--------------------\n",
            "  -> Extracting text from 2509.09587v1...\n",
            "  -> Extracted 33707 characters.\n",
            "  -> Final Score for 2509.09587v1: -128\n",
            "--------------------\n",
            "  -> Extracting text from 2509.09573v1...\n",
            "  -> Extracted 43417 characters.\n",
            "  -> Final Score for 2509.09573v1: 15\n",
            "--------------------\n",
            "  -> Extracting text from 2509.09557v1...\n",
            "  -> Extracted 115803 characters.\n",
            "  -> Final Score for 2509.09557v1: -24\n",
            "--------------------\n",
            "  -> Extracting text from 2509.09538v1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P17' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H1' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H1' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H1' is an invalid float value\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Extracted 49086 characters.\n",
            "  -> Final Score for 2509.09538v1: -56\n",
            "--------------------\n",
            "  -> Extracting text from 2509.09517v1...\n",
            "  -> Extracted 83519 characters.\n",
            "  -> Final Score for 2509.09517v1: -245\n",
            "--------------------\n",
            "  -> Extracting text from 2509.09477v1...\n",
            "  -> Extracted 27238 characters.\n",
            "  -> Final Score for 2509.09477v1: 4\n",
            "--------------------\n",
            "  -> Extracting text from 2509.09476v1...\n",
            "  -> Extracted 62728 characters.\n",
            "  -> Final Score for 2509.09476v1: -204\n",
            "--------------------\n",
            "  -> Extracting text from 2509.09465v1...\n",
            "  -> Extracted 86300 characters.\n",
            "  -> Final Score for 2509.09465v1: 38\n",
            "--------------------\n",
            "  -> Extracting text from 2509.09464v1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pdfminer.pdfinterp:Cannot set gray stroke color because /'P0' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray stroke color because /'P1' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray stroke color because /'P0' is an invalid float value\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Extracted 96338 characters.\n",
            "  -> Final Score for 2509.09464v1: 936\n",
            "--------------------\n",
            "  -> Extracting text from 2509.09432v1...\n",
            "  -> Extracted 49948 characters.\n",
            "  -> Final Score for 2509.09432v1: 170\n",
            "--------------------\n",
            "  -> Extracting text from 2509.09423v1...\n",
            "  -> Extracted 36582 characters.\n",
            "  -> Final Score for 2509.09423v1: -25\n",
            "--------------------\n",
            "  -> Extracting text from 2509.09421v1...\n",
            "  -> Extracted 83154 characters.\n",
            "  -> Final Score for 2509.09421v1: -184\n",
            "--------------------\n",
            "  -> Extracting text from 2509.09402v1...\n",
            "  -> Extracted 32077 characters.\n",
            "  -> Final Score for 2509.09402v1: -76\n",
            "\n",
            "Digest generated! See the file 'benchmark_digest.html' in the file browser on the left.\n"
          ]
        }
      ]
    }
  ]
}